---

description: "Task list for Test Harness feature implementation"
---

# Tasks: Test Harness

**Input**: Design documents from `/workspace/specs/003-test-harness/`
**Prerequisites**: plan.md ‚úì, spec.md ‚úì, research.md ‚úì, data-model.md ‚úì, contracts/cli-interface.md ‚úì, quickstart.md ‚úì

**Tests**: Per constitutional principle I (TDD), tests are written FIRST where they enable the development approach. For this test harness, self-testing via passthrough scenarios validates the entire system.

**Organization**: Tasks are grouped by user story to enable independent implementation and testing of each story.

## Format: `- [ ] [ID] [P?] [Story?] Description`

- **Checkbox**: All tasks start with `- [ ]` (markdown checkbox)
- **[ID]**: Sequential task number (T001, T002, T003...)
- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (US1, US2, US3, US4)
- **Description**: Clear action with exact file path

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: Project initialization and dependency configuration

- [ ] T001 Add polars-testing dependency to crates/cli/Cargo.toml and crates/test-resolver/Cargo.toml
- [ ] T002 Add walkdir dependency to crates/cli/Cargo.toml for test suite discovery
- [ ] T003 [P] Create test scenarios directory structure: tests/scenarios/ and tests/scenarios/.snapshots/
- [ ] T004 [P] Verify uuid v7 feature enabled in /workspace/Cargo.toml dependencies
- [ ] T005 [P] Verify chrono dependency available in /workspace/Cargo.toml

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: Core entity models and trait definitions that ALL user stories depend on

**‚ö†Ô∏è CRITICAL**: No user story work can begin until this phase is complete

- [ ] T006 Create TestScenario entity struct in crates/core/src/model/test_scenario.rs with serde derives
- [ ] T007 [P] Create PeriodDef entity struct in crates/core/src/model/test_scenario.rs
- [ ] T008 [P] Create TestInput entity struct in crates/core/src/model/test_scenario.rs
- [ ] T009 [P] Create DataBlock entity struct in crates/core/src/model/test_scenario.rs
- [ ] T010 [P] Create ProjectDef enum in crates/core/src/model/test_scenario.rs (Inline variant and Ref variant)
- [ ] T011 [P] Create TestOutput entity struct in crates/core/src/model/test_scenario.rs
- [ ] T012 [P] Create TestConfig entity struct in crates/core/src/model/test_scenario.rs with default values
- [ ] T013 [P] Create MatchMode enum in crates/core/src/model/test_scenario.rs (Exact, Subset variants)
- [ ] T014 [P] Create TestResult entity struct in crates/core/src/model/test_scenario.rs
- [ ] T015 [P] Create TestStatus enum in crates/core/src/model/test_scenario.rs (Pass, Fail, Error variants)
- [ ] T016 [P] Create DataMismatch entity struct in crates/core/src/model/test_scenario.rs
- [ ] T017 [P] Create MismatchType enum in crates/core/src/model/test_scenario.rs (MissingRow, ExtraRow, ValueMismatch)
- [ ] T018 [P] Create TraceAssertion entity struct in crates/core/src/model/test_scenario.rs
- [ ] T019 [P] Create TraceMismatch entity struct in crates/core/src/model/test_scenario.rs
- [ ] T020 [P] Create TraceMismatchType enum in crates/core/src/model/test_scenario.rs
- [ ] T021 [P] Create ErrorDetail entity struct in crates/core/src/model/test_scenario.rs
- [ ] T022 [P] Create ErrorType enum in crates/core/src/model/test_scenario.rs
- [ ] T023 Implement TestScenario::validate() method in crates/core/src/model/test_scenario.rs (DataBlock one-of constraint, period validation)
- [ ] T024 [P] Export all test entities from crates/core/src/model/mod.rs
- [ ] T025 Create test-resolver crate lib.rs with public module structure in crates/test-resolver/src/lib.rs

**Checkpoint**: Foundation ready - user story implementation can now begin in parallel

---

## Phase 3: User Story 1 - Execute Single Test Scenario (Priority: P1) üéØ MVP

**Goal**: Parse YAML scenario, inject metadata, execute pipeline, compare output, report pass/fail

**Independent Test**: Create passthrough scenario YAML, run `dobo test <file>`, verify test passes with correct metadata and output comparison

### Implementation for User Story 1

**Core Data Loading & Metadata Injection (test-resolver crate)**

- [ ] T026 [P] [US1] Create InMemoryDataLoader struct in crates/test-resolver/src/loader.rs implementing DataLoader trait
- [ ] T027 [P] [US1] Implement load() method in crates/test-resolver/src/loader.rs to serve LazyFrame from HashMap
- [ ] T028 [P] [US1] Create InMemoryMetadataStore struct in crates/test-resolver/src/metadata.rs implementing MetadataStore trait
- [ ] T029 [P] [US1] Implement get_dataset/get_project/get_run methods in crates/test-resolver/src/metadata.rs
- [ ] T030 [P] [US1] Create InMemoryTraceWriter struct in crates/test-resolver/src/trace.rs implementing TraceWriter trait
- [ ] T031 [US1] Create inject_system_metadata() function in crates/test-resolver/src/injection.rs generating UUIDs v7 and timestamps
- [ ] T032 [US1] Add inject_temporal_metadata() logic in crates/test-resolver/src/injection.rs based on temporal_mode
- [ ] T033 [US1] Create build_lazyframe() helper in crates/test-resolver/src/loader.rs to convert DataBlock rows to Polars LazyFrame
- [ ] T034 [US1] Add CSV file loading support in crates/test-resolver/src/loader.rs using polars::prelude::LazyCsvReader
- [ ] T035 [US1] Add Parquet file loading support in crates/test-resolver/src/loader.rs using polars::prelude::LazyFrame::scan_parquet

**YAML Parsing (cli crate)**

- [ ] T036 [P] [US1] Create parse_scenario() function in crates/cli/src/harness/parser.rs using serde_yaml deserialization
- [ ] T037 [US1] Add error handling with anyhow::Context in crates/cli/src/harness/parser.rs for file read and YAML parse errors
- [ ] T038 [US1] Call TestScenario::validate() after parsing in crates/cli/src/harness/parser.rs

**Output Comparison Engine (cli crate)**

- [ ] T039 [P] [US1] Create compare_output() function in crates/cli/src/harness/comparator.rs accepting actual/expected DataFrames and MatchMode
- [ ] T040 [US1] Implement Exact match mode in crates/cli/src/harness/comparator.rs using polars_testing::assert_dataframe_equal with order-insensitive option
- [ ] T041 [US1] Implement panic capture in crates/cli/src/harness/comparator.rs to collect mismatches from assert_dataframe_equal failures
- [ ] T042 [US1] Create parse_polars_error() helper in crates/cli/src/harness/comparator.rs to extract mismatch details from panic message
- [ ] T043 [US1] Implement strip_system_columns() in crates/cli/src/harness/comparator.rs to remove _row_id, _created_at, etc. unless validate_metadata=true
- [ ] T044 [US1] Create DataMismatch instances in crates/cli/src/harness/comparator.rs for each detected mismatch

**Pipeline Execution (cli crate)**

- [ ] T045 [US1] Create execute_pipeline_mock() function in crates/cli/src/harness/executor.rs returning input unchanged (passthrough stub)
- [ ] T046 [US1] Add TODO comment in crates/cli/src/harness/executor.rs to replace with core::engine::execute_pipeline when S10 available

**Test Orchestration (cli crate)**

- [ ] T047 [US1] Create execute_scenario() function in crates/cli/src/harness/executor.rs orchestrating full test execution flow
- [ ] T048 [US1] Instantiate InMemoryDataLoader in crates/cli/src/harness/executor.rs with test data
- [ ] T049 [US1] Call execute_pipeline_mock() in crates/cli/src/harness/executor.rs with loaded data
- [ ] T050 [US1] Call compare_output() in crates/cli/src/harness/executor.rs with actual vs expected data
- [ ] T051 [US1] Assemble TestResult in crates/cli/src/harness/executor.rs with status, warnings, mismatches

**Result Reporting (cli crate)**

- [ ] T052 [P] [US1] Create report_result() function in crates/cli/src/harness/reporter.rs for human-readable output
- [ ] T053 [US1] Format PASS output in crates/cli/src/harness/reporter.rs showing row counts and validation summary
- [ ] T054 [US1] Format FAIL output in crates/cli/src/harness/reporter.rs listing all data mismatches
- [ ] T055 [US1] Format ERROR output in crates/cli/src/harness/reporter.rs showing error type and message
- [ ] T056 [US1] Create save_snapshot() function in crates/cli/src/harness/reporter.rs to write actual output YAML when snapshot_on_failure=true

**CLI Integration (cli crate)**

- [ ] T057 [P] [US1] Create dobo test command struct in crates/cli/src/commands/test.rs with clap arguments
- [ ] T058 [US1] Add scenario_path argument in crates/cli/src/commands/test.rs (PathBuf, required)
- [ ] T059 [US1] Add --verbose, --no-snapshot, --output options in crates/cli/src/commands/test.rs
- [ ] T060 [US1] Implement command handler in crates/cli/src/commands/test.rs calling parse_scenario() and execute_scenario()
- [ ] T061 [US1] Map TestStatus to exit codes in crates/cli/src/commands/test.rs (Pass=0, Fail=1, Error=2)
- [ ] T062 [US1] Register test command in crates/cli/src/main.rs CLI app

**Self-Test Validation**

- [ ] T063 [US1] Create tests/scenarios/harness-self-test.yaml passthrough scenario from quickstart.md
- [ ] T064 [US1] Run dobo test tests/scenarios/harness-self-test.yaml and verify PASS status

**Checkpoint**: At this point, User Story 1 (single scenario execution) should be fully functional and testable independently

---

## Phase 4: User Story 2 - Validate with Multiple Match Modes (Priority: P2)

**Goal**: Support exact and subset match modes for flexible output validation

**Independent Test**: Create two scenarios (exact mode fails on extra rows, subset mode allows extra rows), verify behavior

### Implementation for User Story 2

- [ ] T065 [P] [US2] Implement Subset match mode in crates/cli/src/harness/comparator.rs using DataFrame left_anti_join
- [ ] T066 [US2] Add find_missing_rows() helper in crates/cli/src/harness/comparator.rs for subset mode
- [ ] T067 [US2] Update compare_output() in crates/cli/src/harness/comparator.rs to branch on MatchMode enum
- [ ] T068 [US2] Create unit test in crates/cli/src/harness/comparator.rs verifying exact mode detects extra rows
- [ ] T069 [US2] Create unit test in crates/cli/src/harness/comparator.rs verifying subset mode allows extra rows
- [ ] T070 [US2] Create tests/scenarios/exact-match-test.yaml scenario with match_mode: exact
- [ ] T071 [US2] Create tests/scenarios/subset-match-test.yaml scenario with match_mode: subset
- [ ] T072 [US2] Run both test scenarios and verify correct pass/fail behavior

**Checkpoint**: At this point, User Stories 1 AND 2 should both work independently

---

## Phase 5: User Story 3 - Validate Trace Events (Priority: P3)

**Goal**: Validate pipeline execution produces expected trace events for traceability verification

**Independent Test**: Create scenario with validate_traceability: true and trace_assertions, verify trace validation

### Implementation for User Story 3

- [ ] T073 [P] [US3] Create validate_trace_events() function in crates/cli/src/harness/comparator.rs
- [ ] T074 [US3] Implement trace event matching in crates/cli/src/harness/comparator.rs by operation_order and change_type
- [ ] T075 [US3] Add row_match validation in crates/cli/src/harness/comparator.rs matching HashMap columns
- [ ] T076 [US3] Add expected_diff validation in crates/cli/src/harness/comparator.rs for Updated change_type
- [ ] T077 [US3] Create TraceMismatch instances in crates/cli/src/harness/comparator.rs for missing/extra/diff mismatches
- [ ] T078 [US3] Integrate validate_trace_events() in crates/cli/src/harness/executor.rs when validate_traceability=true
- [ ] T079 [US3] Add trace mismatches to TestResult in crates/cli/src/harness/executor.rs
- [ ] T080 [US3] Update report_result() in crates/cli/src/harness/reporter.rs to display trace mismatches
- [ ] T081 [US3] Create tests/scenarios/trace-validation-test.yaml scenario with trace assertions
- [ ] T082 [US3] Run trace validation scenario tests/scenarios/trace-validation-test.yaml and verify trace mismatch reporting in crates/cli/src/harness/reporter.rs

**Checkpoint**: All user stories 1, 2, and 3 should now be independently functional

---

## Phase 6: User Story 4 - Execute Test Suite (Priority: P3)

**Goal**: Execute all test scenarios in a directory with aggregated reporting

**Independent Test**: Create directory with multiple scenarios, run `dobo test --suite <dir>`, verify aggregated results

### Implementation for User Story 4

- [ ] T083 [P] [US4] Create discover_scenarios() function in crates/cli/src/harness/executor.rs using walkdir
- [ ] T084 [US4] Implement recursive YAML file search in crates/cli/src/harness/executor.rs with **/*.yaml and **/*.yml patterns
- [ ] T085 [US4] Filter out hidden files and underscore-prefixed files in crates/cli/src/harness/executor.rs
- [ ] T086 [US4] Sort discovered scenarios in crates/cli/src/harness/executor.rs for deterministic order
- [ ] T087 [US4] Create execute_suite() function in crates/cli/src/harness/executor.rs iterating over scenarios
- [ ] T088 [US4] Collect individual TestResult for each scenario in crates/cli/src/harness/executor.rs
- [ ] T089 [US4] Create SuiteResult struct in crates/core/src/model/test_scenario.rs with total/passed/failed/errors counts
- [ ] T090 [US4] Implement report_suite_result() in crates/cli/src/harness/reporter.rs with summary output
- [ ] T091 [US4] Add --suite <directory> flag in crates/cli/src/commands/test.rs
- [ ] T092 [US4] Update command handler in crates/cli/src/commands/test.rs to branch on suite vs single scenario mode
- [ ] T093 [US4] Map suite results to exit codes in crates/cli/src/commands/test.rs (all passed=0, any failed=1, any error=2)
- [ ] T094 [US4] Create tests/scenarios/suite-test/ directory with 3+ test scenarios
- [ ] T095 [US4] Run dobo test --suite tests/scenarios/suite-test/ and verify aggregated reporting

**Checkpoint**: All user stories should now be independently functional

---

## Phase 7: Polish & Cross-Cutting Concerns

**Purpose**: Improvements that affect multiple user stories and final validation

- [ ] T096 [P] Add order_sensitive support in crates/cli/src/harness/comparator.rs using DataFrameEqualOptions.with_check_row_order()
- [ ] T097 [P] Add JSON output format in crates/cli/src/harness/reporter.rs for --output json
- [ ] T098 [P] Add JUnit XML output format in crates/cli/src/harness/reporter.rs for --output junit
- [ ] T099 [P] Add ProjectRef version drift warning in crates/cli/src/harness/executor.rs when project versions differ
- [ ] T100 [P] Improve error messages in crates/cli/src/harness/parser.rs with file location and field paths
- [ ] T101 [P] Add verbose mode implementation in crates/cli/src/harness/reporter.rs showing all mismatches not just summary
- [ ] T102 [P] Create comprehensive unit tests for inject_system_metadata() in crates/test-resolver/src/injection.rs
- [ ] T103 [P] Create comprehensive unit tests for TestScenario::validate() in crates/core/src/model/test_scenario.rs
- [ ] T104 [P] Add integration test for external CSV file loading in crates/test-resolver/src/loader.rs
- [ ] T105 [P] Add integration test for external Parquet file loading in crates/test-resolver/src/loader.rs
- [ ] T106 Validate all /workspace/specs/003-test-harness/quickstart.md examples work end-to-end
- [ ] T107 Run cargo test for workspace defined by /workspace/Cargo.toml and verify all tests pass
- [ ] T108 Run cargo clippy for workspace defined by /workspace/Cargo.toml and fix any warnings
- [ ] T109 Run cargo fmt for workspace defined by /workspace/Cargo.toml to ensure formatting consistency
- [ ] T110 Update /workspace/README.md with test harness usage examples

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: No dependencies - can start immediately
- **Foundational (Phase 2)**: Depends on Setup completion - BLOCKS all user stories
- **User Stories (Phase 3-6)**: All depend on Foundational phase completion
  - User stories CAN proceed in parallel (if staffed)
  - Or sequentially in priority order (US1 ‚Üí US2 ‚Üí US3 ‚Üí US4)
- **Polish (Phase 7)**: Depends on all user stories being complete

### User Story Dependencies

- **User Story 1 (P1)**: Can start after Foundational (Phase 2) - No dependencies on other stories - **MVP SCOPE**
- **User Story 2 (P2)**: Can start after Foundational (Phase 2) - No dependencies on US1 (extends comparator independently)
- **User Story 3 (P3)**: Can start after Foundational (Phase 2) - No dependencies on US1/US2 (adds trace validation independently)
- **User Story 4 (P3)**: Can start after Foundational (Phase 2) - No dependencies on US1/US2/US3 (adds suite orchestration independently)

### Within Each User Story

- Entity models before services
- Services before pipeline execution
- Core implementation before CLI integration
- Story complete before moving to next priority

### Parallel Opportunities

- **Setup (Phase 1)**: Tasks T003, T004, T005 can run in parallel
- **Foundational (Phase 2)**: Tasks T007-T022 can run in parallel (all entity definitions)
- **User Story 1**: 
  - Tasks T026-T027, T028-T029, T030, T036, T039, T052, T057 can run in parallel (different files)
  - Models: T026-T027 (loader), T028-T029 (metadata), T030 (trace) in parallel
- **User Story 2**: Tasks T065, T068, T069 can run in parallel
- **User Story 3**: Tasks T073, T081 can start in parallel
- **User Story 4**: Tasks T083, T089 can run in parallel
- **Polish (Phase 7)**: Tasks T096-T105 can run in parallel (all different files/modules)

**Different user stories can be worked on in parallel by different team members after Phase 2 completes**

---

## Parallel Example: User Story 1

```bash
# After Foundational phase completes, launch all core components together:

# Data loading components (different files, no dependencies):
Task: "Create InMemoryDataLoader struct in crates/test-resolver/src/loader.rs"
Task: "Create InMemoryMetadataStore struct in crates/test-resolver/src/metadata.rs"
Task: "Create InMemoryTraceWriter struct in crates/test-resolver/src/trace.rs"

# Harness components (different files, no dependencies):
Task: "Create parse_scenario() function in crates/cli/src/harness/parser.rs"
Task: "Create compare_output() function in crates/cli/src/harness/comparator.rs"
Task: "Create report_result() function in crates/cli/src/harness/reporter.rs"
Task: "Create dobo test command struct in crates/cli/src/commands/test.rs"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup ‚Üí Dependencies configured
2. Complete Phase 2: Foundational ‚Üí All entity models ready (CRITICAL - blocks all stories)
3. Complete Phase 3: User Story 1 ‚Üí Single scenario execution working
4. **STOP and VALIDATE**: Test User Story 1 independently with passthrough scenario
5. Deploy/demo basic test harness capability

**MVP Delivers**: Ability to run single YAML test scenarios with metadata injection, passthrough pipeline execution, and exact output comparison.

### Incremental Delivery

1. **Foundation** (Setup + Foundational) ‚Üí Entity models and project structure ready
2. **MVP** (+ User Story 1) ‚Üí Single scenario execution ‚Üí Test independently ‚Üí Deploy/Demo
3. **v1.1** (+ User Story 2) ‚Üí Multiple match modes ‚Üí Test independently ‚Üí Deploy/Demo
4. **v1.2** (+ User Story 3) ‚Üí Trace validation ‚Üí Test independently ‚Üí Deploy/Demo
5. **v1.3** (+ User Story 4) ‚Üí Suite execution ‚Üí Test independently ‚Üí Deploy/Demo
6. **v1.4** (+ Polish) ‚Üí Production-ready harness

Each story adds value without breaking previous stories.

### Parallel Team Strategy

With multiple developers after Phase 2 completes:

- **Developer A**: User Story 1 (T026-T064) - Core single scenario execution
- **Developer B**: User Story 2 (T065-T072) - Match mode variants
- **Developer C**: User Story 3 (T073-T082) - Trace validation
- **Developer D**: User Story 4 (T083-T095) - Suite execution

Stories complete and integrate independently.

---

## Task Summary

- **Total Tasks**: 110
- **Setup Phase**: 5 tasks
- **Foundational Phase**: 20 tasks (blocking)
- **User Story 1 (P1 - MVP)**: 39 tasks
- **User Story 2 (P2)**: 8 tasks
- **User Story 3 (P3)**: 10 tasks
- **User Story 4 (P3)**: 13 tasks
- **Polish Phase**: 15 tasks

**Parallel Opportunities**: 52 tasks marked [P] can run in parallel within their phase

**MVP Scope**: Phase 1 + Phase 2 + Phase 3 (64 tasks total) delivers minimum viable test harness

---

## Notes

- All tasks follow strict checklist format: `- [ ] T### [P?] [Story?] Description with file path`
- [P] tasks target different files with no dependencies and can run in parallel
- [Story] label (US1-US4) maps each task to specific user story for traceability
- Each user story is independently completable and testable
- Foundational phase MUST complete before any user story work begins
- Test scenarios validate the test harness itself (self-hosting approach)
- Pipeline execution is stubbed (passthrough) until S10 is implemented
- Commit after each task or logical group
- Stop at any checkpoint to validate story independently
- Format validation: ALL tasks use checkbox `- [ ]`, task ID, optional [P]/[Story] labels, and file paths
